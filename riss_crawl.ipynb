{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 크롬 드라이버 다운 필요\n",
    "(크롬이 최신 버전이라는 전제하에 시작, 작업 디렉토리까지의 절대경로에 한글이 있으면 오류가 발생할 수 있음)\n",
    "1. 크롬 접속 후 우측 상단 점 3개 클릭 → 도움말 → Chrome 정보 클릭\n",
    "2. 자신의 Chrome 버전 확인\n",
    "3. 링크 접속 https://googlechromelabs.github.io/chrome-for-testing/\n",
    "4. 자신과 똑같거나 거의 비슷한 버전의 왼쪽 단어 클릭(Stable, Beta, Dev, Canary 중 하나)\n",
    "5. chromedriver 항목중에서 자신의 OS 환경에 맞는 URL 주소 복사\n",
    "6. 인터넷 창에 URL 복사 붙여넣기하면 다운받을 수 있음\n",
    "7. 자신의 작업 디렉토리에 압축 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 터미널에 붙여넣기\n",
    "# pip install bs4 selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 검색어 설정\n",
    "search_keyword = \"LLM\"\n",
    "\n",
    "# 작업 디렉토리까지의 절대경로 입력 + 마지막에 / 입력\n",
    "load_folder = 'C:/Users/kimsu/Desktop/crawling/'\n",
    "\n",
    "# 데이터를 저장할 파일 이름 설정\n",
    "crawling_data_file_name = f'riss_crawling_{search_keyword}.csv'\n",
    "\n",
    "# 직접 검색하여 총 몇 페이지인지 확인 후 입력\n",
    "page_count = 46                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import getpass\n",
    "import time\n",
    "\n",
    "# URL 설정\n",
    "URL = f\"https://www.riss.kr/search/Search.do?isDetailSearch=N&searchGubun=true&viewYn=OP&query={search_keyword}&queryText=&iStartCount=0&iGroupView=5&icate=all&colName=re_a_kor&exQuery=&exQueryText=&order=%2FDESC&onHanja=false&strSort=RANK&pageScale=10&orderBy=&fsearchMethod=search&isFDetailSearch=N&sflag=1&searchQuery=LLM&fsearchSort=&fsearchOrder=&limiterList=&limiterListText=&facetList=&facetListText=&fsearchDB=&resultKeyword={search_keyword}&pageNumber=1&p_year1=&p_year2=&dorg_storage=&mat_type=&mat_subtype=&fulltext_kind=&t_gubun=&learning_type=&language_code=&ccl_code=&language=&inside_outside=&fric_yn=&db_type=&image_yn=&regnm=&gubun=&kdc=&ttsUseYn=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 페이지를 돌면서 링크를 가져오는 함수\n",
    "def get_paper_links(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # 페이지 로딩을 기다립니다.\n",
    "\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    links = []\n",
    "\n",
    "    for a in soup.select(\"#divContent > div > div.rightContent.wd756 > div > div.srchResultW > div.srchResultListW > ul > li > div.cont.ml60 > p.title > a\"):\n",
    "        links.append('https://www.riss.kr' + a['href'])\n",
    "\n",
    "    return links\n",
    "\n",
    "# 논문 세부 정보를 가져오는 함수\n",
    "def get_paper_details(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # 페이지 로딩을 기다립니다.\n",
    "\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('h3.title')\n",
    "    title_kor = title.get_text(strip=True) if title else \"\"\n",
    "    \n",
    "    authors = soup.select_one('#thesisInfoDiv > div.infoDetail.on > div.infoDetailL > ul > li:nth-of-type(1) > div > p')\n",
    "    authors_text = authors.get_text(strip=True) if authors else \"\"\n",
    "    \n",
    "    keywords = soup.select_one('#thesisInfoDiv > div.infoDetail.on > div.infoDetailL > ul > li:nth-of-type(7) > div > p')\n",
    "    keywords_text = keywords.get_text(strip=True) if keywords else \"\"\n",
    "\n",
    "    multilingual_abstract = soup.select_one('#additionalInfoDiv > div > div:nth-child(1) > div:nth-child(4) > p')\n",
    "    multilingual_abstract_text = multilingual_abstract.get_text(strip=True) if multilingual_abstract else \"\"\n",
    "\n",
    "    abstract = soup.select_one('#additionalInfoDiv > div > div:nth-child(2) > div:nth-child(4) > p')\n",
    "    abstract_text = abstract.get_text(strip=True) if abstract else \"\"\n",
    "\n",
    "    year = soup.select_one('#thesisInfoDiv > div.infoDetail.on > div.infoDetailL > ul > li:nth-child(5) > div > p')\n",
    "    year_text = year.get_text(strip=True) if year else \"\"\n",
    "\n",
    "    return {\n",
    "        '저자': authors_text,\n",
    "        '제목': title_kor,\n",
    "        '키워드': keywords_text,\n",
    "        '발행연도': year_text,\n",
    "        '요약': multilingual_abstract_text + '\\n' + abstract_text, # 국문과 영문의 위치가 논문마다 달라서 합침\n",
    "        '링크': url\n",
    "    }\n",
    "\n",
    "# 데이터를 CSV 파일에 저장하는 함수\n",
    "def save_to_csv(data, csv_path):\n",
    "    df = pd.DataFrame(data)\n",
    "    if os.path.isfile(csv_path):\n",
    "        df.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(csv_path, mode='w', header=True, index=False)\n",
    "\n",
    "# 폴더를 생성하는 함수\n",
    "def make_folder(folder_name):\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    driver_path = 'chromedriver-win64/chromedriver.exe' # 압축 해제한 폴더\n",
    "    chrome_service = Service(driver_path)\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"headless\")\n",
    "    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)\n",
    "\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    user_name = getpass.getuser()\n",
    "    folder_root = load_folder\n",
    "    path = folder_root + now + '/'\n",
    "    make_folder(path)\n",
    "\n",
    "    csv_path = path + crawling_data_file_name\n",
    "    all_paper_details = []\n",
    "\n",
    "    start_page = 1\n",
    "    end_page = page_count\n",
    "\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        iStartCount = (page - 1) * 10  # 페이지당 10개씩\n",
    "        # pageNumber, iStartCount 값에 맞춰 URL 생성\n",
    "        URL = (f\"https://www.riss.kr/search/Search.do?\"\n",
    "               f\"isDetailSearch=N&searchGubun=true&viewYn=OP&query={search_keyword}\"\n",
    "               f\"&queryText=&iStartCount={iStartCount}&iGroupView=5&icate=all\"\n",
    "               f\"&colName=re_a_kor&exQuery=&exQueryText=&order=%2FDESC&onHanja=false\"\n",
    "               f\"&strSort=RANK&pageScale=10&orderBy=&fsearchMethod=search\"\n",
    "               f\"&isFDetailSearch=N&sflag=1&searchQuery={search_keyword}\"\n",
    "               f\"&fsearchSort=&fsearchOrder=&limiterList=&limiterListText=\"\n",
    "               f\"&facetList=&facetListText=&fsearchDB=&resultKeyword={search_keyword}\"\n",
    "               f\"&pageNumber={page}&p_year1=&p_year2=&dorg_storage=&mat_type=\"\n",
    "               f\"&mat_subtype=&fulltext_kind=&t_gubun=&learning_type=&language_code=\"\n",
    "               f\"&ccl_code=&language=&inside_outside=&fric_yn=&db_type=&image_yn=\"\n",
    "               f\"&regnm=&gubun=&kdc=&ttsUseYn=\")\n",
    "\n",
    "        paper_links = get_paper_links(driver, URL)\n",
    "\n",
    "        # 2) 각 페이지별로 추출한 링크에 대해서 상세 정보 수집\n",
    "        for link in paper_links:\n",
    "            paper_details = get_paper_details(driver, link)\n",
    "            all_paper_details.append(paper_details)\n",
    "\n",
    "    # 3) 모든 페이지에 대해 수집한 결과를 CSV로 저장\n",
    "    save_to_csv(all_paper_details, csv_path)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "# 실행하는 동안 컴퓨터가 절전모드에 들어가면 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
